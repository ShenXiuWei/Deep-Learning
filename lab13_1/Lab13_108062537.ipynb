{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujdmCh3bxkP4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STQiHeKfyXka"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7A63W8RBygRG"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "movie_reviews = pd.read_csv(\"./datasets/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2529,
     "status": "ok",
     "timestamp": 1574439464563,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "4qttHktWy7ny",
    "outputId": "3c47b553-9c8d-4bec-9c1e-689010ceddfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any null value in the dataset\n",
    "movie_reviews.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2451,
     "status": "ok",
     "timestamp": 1574439465732,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "vNXNa8KRzCka",
    "outputId": "5c6cfe96-a43a-4ce4-96c8-97cdc5d683a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the size of the dataset\n",
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1271,
     "status": "ok",
     "timestamp": 1574439465733,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "RJVvK_erzID7",
    "outputId": "68a8777c-789c-43b7-d604-d1a93ddd6560"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first five data in the dataset\n",
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2295,
     "status": "ok",
     "timestamp": 1574439468557,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "7mN9m3kGzLKG",
    "outputId": "ca9c8483-a15a-4985-c16e-65b783d671b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews[\"review\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zne2wuKhzNz0"
   },
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmenw_JezQll"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "\n",
    "# replace the positive with 1, replace the negative with 0\n",
    "y = movie_reviews['sentiment']\n",
    "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RTXf_32zSdk"
   },
   "outputs": [],
   "source": [
    "# Split the training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWqJNuoQzUvb"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "\n",
    "# padding sentences to the same length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train, padding='post', maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14924,
     "status": "ok",
     "timestamp": 1574439498321,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "eS8IqcKlzW0n",
    "outputId": "468acf15-b7fc-4791-f8d9-01abfd1ebf01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100,   20,  155,   81,   17,   46,   14,    1,   98, 4696,    2,\n",
       "       2508, 1560,    2,    1,  864,    1,  100,  639,   51,    7,    5,\n",
       "         19,  188,  403,    1,  546,    9,   21,    5, 4342,   51,    7,\n",
       "       1810,  585,    4, 2408,  417,   33,    1,  296,   41, 1264,   63,\n",
       "         19,  250,  137,   30, 1069,  100,   31,  107,   60,   14,    1,\n",
       "         76,   98,    7,    1,   12,    2,  628, 4808,    5, 1082, 6747,\n",
       "        541,   13,  259,    4, 2408,  897,    2,    1,  204,  132, 1181,\n",
       "         32,  700,    2, 7163,   26,    1,  603,  455,  220,   94, 1052,\n",
       "          8,   12,   91,   23,   71, 1681,   15,    6,  211,   82,   99,\n",
       "          6], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the preprocessed data\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13655,
     "status": "ok",
     "timestamp": 1574439498322,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "AjmpKcU9zYof",
    "outputId": "78c802af-fd7f-4f4a-a4db-a8705405cc4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 100]), TensorShape([64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "# only reserve 10000 words\n",
    "vocab_size = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf4NBjkjzbX8"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # x is the training data，shape == (batch_size，max_length)  -> (64, 100)\n",
    "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
    "        # hidden state shape == (batch_size, units) -> (64, 1024)\n",
    "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (64, 100, 256)\n",
    "        x = self.embedding(x)\n",
    "        # output contains the state(in GRU, hidden state equals to output in each timestamp) from all timestamps,\n",
    "        # output shape == (batch_size, max_length, units) -> (64, 100, 1024)\n",
    "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (64, 1024)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        # output contains the whole output of the sequence, state is the hidden state of the last timestamp\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # initialize the first state of the gru,  shape == (batch_size, units) -> (64,1024)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9064,
     "status": "ok",
     "timestamp": 1574439499141,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "JuGtI0NRzhbr",
    "outputId": "686e55fd-6532-4c8e-cf47-20d638699285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 100, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
      "tf.Tensor([ True  True  True ...  True  True  True], shape=(1024,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(\n",
    "    sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "# the output and the hidden state of GRU is equal\n",
    "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNIHTMFh0muK"
   },
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6UJqUvgzhUA"
   },
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        a = values\n",
    "        b = tf.transpose(self.W(hidden_with_time_axis), perm=[0,2,1])\n",
    "        score = tf.matmul(a, b)\n",
    "\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-V75CY_zloY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        # through four fully connected layers, the model will return the probability of the positivity of the sentence\n",
    "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
    "        self.fc_2 = tf.keras.layers.Dense(512)\n",
    "        self.fc_3 = tf.keras.layers.Dense(64)\n",
    "        self.fc_4 = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = LuongAttention(self.dec_units)\n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        output = self.fc_1(context_vector)\n",
    "        output = self.fc_2(output)\n",
    "        output = self.fc_3(output)\n",
    "        output = self.fc_4(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4037,
     "status": "ok",
     "timestamp": 1574439499685,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "CATctRBszo2a",
    "outputId": "6faf0e8c-e782-493e-f3ec-c8c031847f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 1)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (batch_size, vocab size) {}'.format(\n",
    "    sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esTgwOeDzqeu"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hb3riNyIzsmM"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/sentiment-analysis-luong-attention'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDY1tUKEzwzm"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, _ = decoder(enc_hidden, enc_output)\n",
    "\n",
    "        loss = loss_function(targ, predictions)\n",
    "\n",
    "    # collect all trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # calculate the gradients for the whole variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # apply the gradients on the variables\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9KsW2DPfzzcc",
    "outputId": "1cb767a9-21c8-45d4-b9cc-9c1a1c8d2ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.6926\n",
      "Epoch 1 Batch 100 Loss 0.3308\n",
      "Epoch 1 Batch 200 Loss 0.4543\n",
      "Epoch 1 Batch 300 Loss 0.2621\n",
      "Epoch 1 Batch 400 Loss 0.2388\n",
      "Epoch 1 Batch 500 Loss 0.3656\n",
      "Epoch 1 Batch 600 Loss 0.3891\n",
      "Epoch 1 Loss 0.3712\n",
      "Time taken for 1 epoch 984.7998483181 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2009\n",
      "Epoch 2 Batch 100 Loss 0.1811\n",
      "Epoch 2 Batch 200 Loss 0.2624\n",
      "Epoch 2 Batch 300 Loss 0.1843\n",
      "Epoch 2 Batch 400 Loss 0.3148\n",
      "Epoch 2 Batch 500 Loss 0.2221\n",
      "Epoch 2 Batch 600 Loss 0.2110\n",
      "Epoch 2 Loss 0.2428\n",
      "Time taken for 1 epoch 975.9246542453766 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1645\n",
      "Epoch 3 Batch 100 Loss 0.1819\n",
      "Epoch 3 Batch 200 Loss 0.2285\n",
      "Epoch 3 Batch 300 Loss 0.1204\n",
      "Epoch 3 Batch 400 Loss 0.2592\n",
      "Epoch 3 Batch 500 Loss 0.2448\n",
      "Epoch 3 Batch 600 Loss 0.1293\n",
      "Epoch 3 Loss 0.1691\n",
      "Time taken for 1 epoch 975.0191013813019 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1300\n",
      "Epoch 4 Batch 100 Loss 0.0965\n",
      "Epoch 4 Batch 200 Loss 0.1341\n",
      "Epoch 4 Batch 300 Loss 0.0627\n",
      "Epoch 4 Batch 400 Loss 0.1329\n",
      "Epoch 4 Batch 500 Loss 0.3157\n",
      "Epoch 4 Batch 600 Loss 0.1064\n",
      "Epoch 4 Loss 0.1093\n",
      "Time taken for 1 epoch 977.4308249950409 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0504\n",
      "Epoch 5 Batch 100 Loss 0.1123\n",
      "Epoch 5 Batch 200 Loss 0.0112\n",
      "Epoch 5 Batch 300 Loss 0.1591\n",
      "Epoch 5 Batch 400 Loss 0.0268\n",
      "Epoch 5 Batch 500 Loss 0.0407\n",
      "Epoch 5 Batch 600 Loss 0.1691\n",
      "Epoch 5 Loss 0.0768\n",
      "Time taken for 1 epoch 977.7448153495789 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0359\n",
      "Epoch 6 Batch 100 Loss 0.0164\n",
      "Epoch 6 Batch 200 Loss 0.1185\n",
      "Epoch 6 Batch 300 Loss 0.0287\n",
      "Epoch 6 Batch 400 Loss 0.2386\n",
      "Epoch 6 Batch 500 Loss 0.0261\n",
      "Epoch 6 Batch 600 Loss 0.0230\n",
      "Epoch 6 Loss 0.0533\n",
      "Time taken for 1 epoch 976.5590350627899 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0082\n",
      "Epoch 7 Batch 100 Loss 0.0013\n",
      "Epoch 7 Batch 200 Loss 0.0253\n",
      "Epoch 7 Batch 300 Loss 0.0324\n",
      "Epoch 7 Batch 400 Loss 0.0773\n",
      "Epoch 7 Batch 500 Loss 0.0019\n",
      "Epoch 7 Batch 600 Loss 0.0378\n",
      "Epoch 7 Loss 0.0439\n",
      "Time taken for 1 epoch 963.0166258811951 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0128\n",
      "Epoch 8 Batch 100 Loss 0.3247\n",
      "Epoch 8 Batch 200 Loss 0.0565\n",
      "Epoch 8 Batch 300 Loss 0.0330\n",
      "Epoch 8 Batch 400 Loss 0.0123\n",
      "Epoch 8 Batch 500 Loss 0.0439\n",
      "Epoch 8 Batch 600 Loss 0.0320\n",
      "Epoch 8 Loss 0.0379\n",
      "Time taken for 1 epoch 976.2393209934235 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0083\n",
      "Epoch 9 Batch 100 Loss 0.0020\n",
      "Epoch 9 Batch 200 Loss 0.0044\n",
      "Epoch 9 Batch 300 Loss 0.0040\n",
      "Epoch 9 Batch 400 Loss 0.1369\n",
      "Epoch 9 Batch 500 Loss 0.0022\n",
      "Epoch 9 Batch 600 Loss 0.0222\n",
      "Epoch 9 Loss 0.0247\n",
      "Time taken for 1 epoch 977.4041197299957 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0132\n",
      "Epoch 10 Batch 100 Loss 0.0035\n",
      "Epoch 10 Batch 200 Loss 0.0383\n",
      "Epoch 10 Batch 300 Loss 0.3508\n",
      "Epoch 10 Batch 400 Loss 0.0273\n",
      "Epoch 10 Batch 500 Loss 0.0336\n",
      "Epoch 10 Batch 600 Loss 0.0420\n",
      "Epoch 10 Loss 0.0223\n",
      "Time taken for 1 epoch 976.166755437851 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0109\n",
      "Epoch 11 Batch 100 Loss 0.0426\n",
      "Epoch 11 Batch 200 Loss 0.0020\n",
      "Epoch 11 Batch 300 Loss 0.0006\n",
      "Epoch 11 Batch 400 Loss 0.0814\n",
      "Epoch 11 Batch 500 Loss 0.0433\n",
      "Epoch 11 Batch 600 Loss 0.0183\n",
      "Epoch 11 Loss 0.0209\n",
      "Time taken for 1 epoch 976.7275702953339 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0477\n",
      "Epoch 12 Batch 100 Loss 0.1676\n",
      "Epoch 12 Batch 200 Loss 0.0007\n",
      "Epoch 12 Batch 300 Loss 0.0285\n",
      "Epoch 12 Batch 400 Loss 0.0020\n",
      "Epoch 12 Batch 500 Loss 0.0308\n",
      "Epoch 12 Batch 600 Loss 0.0952\n",
      "Epoch 12 Loss 0.0231\n",
      "Time taken for 1 epoch 975.0747344493866 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0011\n",
      "Epoch 13 Batch 100 Loss 0.0090\n",
      "Epoch 13 Batch 200 Loss 0.0215\n",
      "Epoch 13 Batch 300 Loss 0.0074\n",
      "Epoch 13 Batch 400 Loss 0.0017\n",
      "Epoch 13 Batch 500 Loss 0.0724\n",
      "Epoch 13 Batch 600 Loss 0.0181\n",
      "Epoch 13 Loss 0.0175\n",
      "Time taken for 1 epoch 975.810632944107 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0077\n",
      "Epoch 14 Batch 100 Loss 0.0048\n",
      "Epoch 14 Batch 200 Loss 0.0297\n",
      "Epoch 14 Batch 300 Loss 0.0001\n",
      "Epoch 14 Batch 400 Loss 0.0351\n",
      "Epoch 14 Batch 500 Loss 0.0061\n",
      "Epoch 14 Batch 600 Loss 0.0031\n",
      "Epoch 14 Loss 0.0147\n",
      "Time taken for 1 epoch 603.1807878017426 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the epochs for training\n",
    "EPOCHS = 14\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # get the initial hidden state of gru\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1725,
     "status": "error",
     "timestamp": 1574430590671,
     "user": {
      "displayName": "聖修魏",
      "photoUrl": "",
      "userId": "12419337041371663462"
     },
     "user_tz": -480
    },
    "id": "89JE_mCQz1nL",
    "outputId": "d1b1cee6-909d-43df-c5ae-918fe4cfa219"
   },
   "outputs": [],
   "source": [
    "# print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# # restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IS4W4imrz3Z9"
   },
   "outputs": [],
   "source": [
    "def evaluate(test_data):\n",
    "\n",
    "    y_predicts = []\n",
    "    attention_weights_list = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "\n",
    "        input_data = tf.expand_dims(tf.convert_to_tensor(test_data[i]), 0)\n",
    "        enc_hidden = [tf.zeros((1, units))]\n",
    "        enc_output, enc_hidden = encoder(input_data, enc_hidden)\n",
    "\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
    "        # convert the tensor to numpy list\n",
    "        attention_weights_list.append(\n",
    "            attention_weights.numpy().flatten().tolist())\n",
    "        y_predicts.append(1 if predictions >= 0.5 else 0)\n",
    "\n",
    "    return y_predicts, attention_weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0ail6LKz5T0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8445\n"
     ]
    }
   ],
   "source": [
    "y_predicts, attention_weights_list = evaluate(X_test)\n",
    "print('Accuracy: ', (y_predicts == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ec5gZIkzz-AV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true:  1\n",
      "y_predict:  0\n",
      "changed matches and and razor jerry vs it ramon ending guard brothers main marty and and vs card and terrible \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "premiere pilot see pilot passing and what the mean see many and if in ever the would that just many \n",
      "\n",
      "y_true:  0\n",
      "y_predict:  0\n",
      "sense by plan may any plot chance characters all guy getting of like movie it the seagal it headache will \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "thompson sister sophie northam mother are as she and and awesome thompson recommended brilliant being law without emma attitude \n",
      "\n",
      "y_true:  0\n",
      "y_predict:  0\n",
      "unique makes it further and imagine wondering to is me touch carell puzzle be victim this in to to reaches \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "most but were time friendly forced boy energetic movie office all spade of and quick this understands and great that \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "film school as this to students death who high images live subtitles with showed film from of numerous to in \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "ve quasi could taken affair tony with sense to on in because me though life otherwise his with at theories \n",
      "\n",
      "y_true:  0\n",
      "y_predict:  0\n",
      "to educational competition between an and us thing tiny supposed be they sense quantum to of clearly accepting since trying \n",
      "\n",
      "y_true:  0\n",
      "y_predict:  0\n",
      "loud moments liners at one unless senses my laughed you and the for out and some while it laughed and \n",
      "\n",
      "y_true:  0\n",
      "y_predict:  0\n",
      "have suspicion dream by below one as talentless this christian storyline movie mediocre the to the did just beautiful live \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "where right rating mood always to back great me doesn part of like losing hard hit in movie boss the \n",
      "\n",
      "y_true:  0\n",
      "y_predict:  0\n",
      "still and everyone made those aired and got movie can movies for movies as one in the act is than \n",
      "\n",
      "y_true:  0\n",
      "y_predict:  0\n",
      "if by graphics it off put haven thing was for made semi charming out japan came \n",
      "\n",
      "y_true:  0\n",
      "y_predict:  0\n",
      "stars feature lundgren shocking film for grade below par all even us films dolph script among had guy is credits \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "status opposed to skip the scenery there comparisons overall this it question are only conservative beautiful helped use movie this \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  0\n",
      "hot sequences current has sequence which but jean an edward its hum on has worth alone is look \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "john in dvd the egg about has person is johnson picked sorts sick of the pink film such alive waters \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "that with much though without effects story excellent gripping music special don note visuals it so brains is significant interesting \n",
      "\n",
      "y_true:  1\n",
      "y_predict:  1\n",
      "that movie looking certainly was this of worth lot correct dr desired of at are your performances negative that \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "# show the twenty most focused words to explain the result\n",
    "# just need to show the first twenty results\n",
    "for index, data in enumerate(X_test[:20]):\n",
    "    print('y_true: ', y_test[index])\n",
    "    print('y_predict: ', y_predicts[index])\n",
    "\n",
    "    # get the twenty biggest attention weights to show\n",
    "    big_weights = heapq.nlargest(20, attention_weights_list[index])\n",
    "\n",
    "    for weight in big_weights:\n",
    "        # get the index of the word\n",
    "        word_index = data[attention_weights_list[index].index(weight)]\n",
    "        if word_index != 0:\n",
    "            # show the twenty most focused words\n",
    "            print(tokenizer.index_word[word_index], end=' ')\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
